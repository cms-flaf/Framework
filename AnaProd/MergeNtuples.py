import ROOT
import os
import shutil
import sys
ROOT.gROOT.SetBatch(True)
ROOT.EnableThreadSafety()

if __name__ == "__main__":
    sys.path.append(os.environ['ANALYSIS_PATH'])


import FLAF.Common.ConvertUproot as ConvertUproot
ROOT.gInterpreter.Declare(
    """
    using lumiEventMapType = std::map < unsigned int, std::set <unsigned long long > >;
    using eventMapType =  std::map < unsigned int ,  lumiEventMapType >;
    using RunLumiEventSet = std::set < std::tuple < unsigned int, unsigned int, unsigned long long > > ;
    namespace kin_fit{
        struct FitResults {
            double mass, chi2, probability;
            int convergence;
            bool HasValidMass() const { return convergence > 0; }
            FitResults() : convergence(std::numeric_limits<int>::lowest()) {}
            FitResults(double _mass, double _chi2, double _probability, int _convergence) :
                mass(_mass), chi2(_chi2), probability(_probability), convergence(_convergence) {}
        };
    }
    bool saveEvent(unsigned int run, unsigned int lumi, unsigned long long event){
        static eventMapType eventMap;
        static std::mutex eventMap_mutex;
        const std::lock_guard<std::mutex> lock(eventMap_mutex);
        auto& events = eventMap[run][lumi];
        if(events.find(event) != events.end())
            return false;
        events.insert(event);
        return true;
        }
    """
)
class ObjDesc:
    def __init__(self, obj_type):
        self.obj_type = obj_type
        self.ref_obj = None
        self.objsToMerge = ROOT.TList()
        self.file_names = []


def merge_ntuples(df):
    if 'run' not in df.GetColumnNames() or 'luminosityBlock'  not in df.GetColumnNames() or 'event'  not in df.GetColumnNames():
        return df
    df = df.Filter("saveEvent(run, luminosityBlock, event)")
    return df

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('inputFile', nargs='+', type=str)
    parser.add_argument('--outFile', required=True, type=str)
    parser.add_argument('--useUproot', type=bool, default=False)
    parser.add_argument('--outFiles', required=False, nargs='+', type=str, default=None)
    args = parser.parse_args()
    headers_dir = os.path.dirname(os.path.abspath(__file__))

    ROOT.gROOT.ProcessLine(f".include {os.environ['ANALYSIS_PATH']}")
    snapshotOptions = ROOT.RDF.RSnapshotOptions()
    snapshotOptions.fOverwriteIfExists=False
    snapshotOptions.fLazy = False
    snapshotOptions.fMode="UPDATE"
    snapshotOptions.fCompressionAlgorithm = getattr(ROOT.ROOT, 'kZLIB')
    snapshotOptions.fCompressionLevel = 4
    not_empty_files = True
    inputFiles = [ (fileName, ROOT.TFile(fileName, "READ")) for fileName in args.inputFile ]
    objects = {}
    for fileName, file in inputFiles:
        for key in file.GetListOfKeys():
            key_name = key.GetName()
            obj = key.ReadObj()

            if obj.IsA().InheritsFrom(ROOT.TTree.Class()):
                objType = "TTree"
            elif obj.IsA().InheritsFrom(ROOT.TH1.Class()):
                objType = "TH1"
            else:
                raise RuntimeError(f"Object {obj} is not a TTree or TH1.")

            if key_name not in objects:
                objects[key_name] = ObjDesc(objType)
            obj_desc = objects[key_name]
            if objType != obj_desc.obj_type:
                raise RuntimeError(f"Object {key_name} is of type {objType} but was previously found to be"
                                f"of type {obj_desc.obj_type}.")
            if objType == "TTree":
                if fileName in obj_desc.file_names:
                    continue # When there were multiple cycles of a tree, without this catch you would append the same file multiple times
                obj_desc.file_names.append(fileName)
            else:
                if obj_desc.ref_obj is None:
                    obj_desc.ref_obj = obj
                else:
                    obj_desc.objsToMerge.Add(obj)

    tmpFileName = args.outFile
    if args.useUproot:
        tmpFileName+= '.tmp.root'
    compression = ROOT.CompressionSettings(snapshotOptions.fCompressionAlgorithm, snapshotOptions.fCompressionLevel)
    # Set final output file name for the TH1's, use only first one now since TH1 can't be split into multiple files
    report_file_name = args.outFiles[0] if args.outFiles is not None and len(args.outFiles) > 0 else tmpFileName
    outputFile = ROOT.TFile(report_file_name, "RECREATE", "", compression)
    for obj_name, obj_desc in objects.items():
        if obj_desc.obj_type != "TH1":
            continue
        obj_desc.ref_obj.Merge(obj_desc.objsToMerge)
        outputFile.WriteTObject(obj_desc.ref_obj, obj_name)
    outputFile.Close()
    for fileName, file in inputFiles:
        file.Close()


    # Delete tmp file if it exists
    outputFile = ROOT.TFile(tmpFileName, "RECREATE", "", compression)
    outputFile.Close()

    for obj_name, obj_desc in objects.items():
        print(f"At writing for {obj_name} {obj_desc}")
        if obj_desc.obj_type != "TTree":
            continue
        print(f"Loading {obj_name}, {obj_desc.file_names}")
        df = ROOT.RDataFrame(obj_name, obj_desc.file_names)
        print(f"initially there are {df.Count().GetValue()} events")
        df = merge_ntuples(df)
        df.Snapshot(obj_name, tmpFileName, df.GetColumnNames(), snapshotOptions)
        #if df.Count().GetValue()==0: not_empty_files=False
    #print(tmpFileName)
    if args.useUproot:
        ConvertUproot.toUproot(tmpFileName, args.outFile)
        #if os.path.exists(args.outFile):
        #    os.remove(tmpFileName)

    if args.outFiles != None and len(args.outFiles) > 0:
        if len(args.outFiles) > 1 and len(objects) > 1:
            raise RuntimeError("Cannot split multiple objects into multiple files, please use a single object or a single file.")
        for obj_name, obj_desc in objects.items():
            print(f"At splitting for {obj_name} {obj_desc}")
            # We want to make this general to also split MC later, but these object names could cause an issue?
            # Is there an edge case where a Events_JERUp event might go to file1, but the Event event might go to file0?
            # For now, this doesn't exist as MC is handle as one outFile per job -- only data has multiple outFiles
            if obj_desc.obj_type != "TTree":
                continue
            df = ROOT.RDataFrame(obj_name, tmpFileName)
            nFiles = len(args.outFiles)
            nEvents = df.Count().GetValue()
            nEventsPerFile = int(nEvents/nFiles)+1 # Add 1 to take care of int() always flooring
            print(f"Going to make int({nEvents}/{nFiles}) {nEventsPerFile} events per file")
            range_start = 0
            for outFileName in args.outFiles:
                df_split = df.Range(range_start, range_start+nEventsPerFile)
                df_split.Snapshot(obj_name, outFileName, df_split.GetColumnNames(), snapshotOptions)
                range_start += nEventsPerFile
